{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Глава 9 Снижение размерности с помощью выделения признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не все признаки создаются равными, и выделение признаков для снижения размерности имеет совершенно конкретную цель - преобразование нашего набора признаков р(исх), чтобы в коненом итоге прийти к новому набору р(нов), где р(исх)>р(нов), сохраняя при этом подавляющуючасть исходной информации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы уменьшим количество признаков с небольшой потерей способности наших данных генерировать высококачественные предсказания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но новые признаки, которые мы генерируем, не поддаются интерпретированию людьми. Они будут содержать почти столько же возможностей для тренировки моделей, но для человеческого глаза будет выглядеть как набор случайных чисел."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если хотим сохранить способность интерпретировать модели, лучше применить другой метод снижения размерности - уменьшения размерности посредством отбора признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Снижение признаков с помощью главных компонент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дан набор признаков, и требуется сократить количество признаков, сохраняя при этом дисперсию данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем анализ главных компонент с помощью класса PCA библиотеки scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузить данные\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'target', 'target_names', 'images', 'DESCR']\n"
     ]
    }
   ],
   "source": [
    "print(list(digits.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# стандартизировать матрицу признаков\n",
    "features = StandardScaler().fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создать объект PCA, который сохранит 99% дисперсии\n",
    "pca = PCA(n_components=0.99, whiten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_pca = pca.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходное количество признаков:  64\n",
      "Сокращенное количество признаков:  54\n"
     ]
    }
   ],
   "source": [
    "print('Исходное количество признаков: ', features.shape[1])\n",
    "print('Сокращенное количество признаков: ', features_pca.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод PCA проецирует наблюдения на главные компоненты матрицы признаков, которые сохраняют наибольшую дисперсию. PCA - является неконтролируемым методом(без учителя) - он не использует информацию из вектора целей и вместо этого рассматривает только матрицу признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У аргумента n_components есть две операции в зависимости от заданного значения, если значение этого аргумента больше 1, то n_components вернет указанное значение признаков. Если n_components находится между 0 и 1, то созданный объект возвращает минимальное количество признаков, которые сохраняют заданную дисперсию. Обычно используют 0.99 и 0.95."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whiten=True   \n",
    "\n",
    "преобразует значение каждой главной компоненты, чтобы они имели нулевое среднее и единичную дисперсию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "svd_solver = \"randomized\"  - реализует стохастический алгоритм нахождения первых главных компонент(занимает меньше времени)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод PCA позволяет уменьшить размерность на 10 признаков, сохраняя 99% информации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Уменьшение количества признаков, когда данные линейно неразделимы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможно ваши данные линейно неразделимы, и требуется сократить размерности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применить расширение анализа главных компонент, в котором используются ядра для нелинейного уменьшения размерности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Загрузить библиотеки\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.datasets import make_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создать линейно-неразделимые данные\n",
    "features, _ = make_circles(n_samples=1000, random_state=1, noise=0.1, factor=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Применить ядерный PCA\n",
    "#с радиально-базисным функциональным ядром(RBF-ядром)\n",
    "kpca = KernelPCA(kernel=\"rbf\", gamma=15, n_components=1)\n",
    "features_kpca = kpca.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходное количество признаков:  2\n",
      "Сокращенное количество признаков:  1\n"
     ]
    }
   ],
   "source": [
    "print('Исходное количество признаков: ', features.shape[1])\n",
    "print('Сокращенное количество признаков: ', features_kpca.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если данные линейно разделимы, то PCA работает хорошо. Если данные не являются линейно-разделимыми, то линейное преобразование работать не будет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция make_circles создает линейно-неразделимые данные (один класс окружен со всех сторон другим классом)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если бы для уменьшения размерностей наших данных мы использовали линейный метод PCA, то два класса были бы линейно спроецированы на первую главную компоненту таким образом, что они стали бы переплетенными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы хотели бы иметь преобразование, которое сокращало бы размерности, а также делало бы данные линейно-разделимыми."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это может делать ядерный PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ядра позволяют проецировать линейно неразделимые данные в более высокую размерность, где они линейно разделимы. Этот подход называется ядерным трюком."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В объекте, созданным из класса KernelPCA библиотеки scikit-learn можно использовать несколько ядер, задаваемых с помощью параметра kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Виды ядер:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)гауссово радиально-базисное функциональное ядро rbf\n",
    "\n",
    "2)полиноминальное ядро poly\n",
    "\n",
    "3)сигмоидное ядро sigmoid\n",
    "\n",
    "4)линейная проекция linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ядерном PCA мы не можем указать n_components как дисперсию. Мы должны задать ряд параметров. Плюс есть еще гиперпараметры(радиально-базисная функция требует значения gamma)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Уменьшение количества признаков путем максимизации разделимости классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Требуется сократить признаки, используемые классификатором"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробовать линейный, дискриминантный анализ (linear discriminant analysis, LDA), чтобы спроецировать объекты на оси компонент, которые максимизируют разделение классов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дискриминантный анализ — это статистический метод, предназначенный для изучения отличий между двумя или большим количеством групп объектов с использованием данных о разнообразии нескольких признаков, отличающих эти объекты друг от друга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ru.wikipedia.org/wiki/%D0%94%D0%B8%D1%81%D0%BA%D1%80%D0%B8%D0%BC%D0%B8%D0%BD%D0%B0%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загрузить набор данных цветков ириса\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создать объект и выполнить LDA, затем использовать\n",
    "#его для преобразования признаков\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "features_lda = lda.fit(features, target).transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходное количество признаков:  4\n",
      "Сокращенное количество признаков:  1\n"
     ]
    }
   ],
   "source": [
    "print('Исходное количество признаков: ', features.shape[1])\n",
    "print('Сокращенное количество признаков: ', features_lda.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# атрибут explained_variance_ratio_    - просмотр объема дисперсии (объясненной каждой компонентой)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9912126])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод LDA работает аналогично анализу главных компонент(PCA): он проецирует пространство признаков на пространство более низкой размерности. Однако в PCA нас интересовали только те оси компонент, которые максимизируют дисперсию данных, в то время как в LDA есть дополнительная цель - максимизировать различия между классами. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_components - указывает на количество признаков, которые требуется вернуть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы выяснить какое знаение использовать в n_components: можно воспользоваться коэффициентом  explained_variance_ratio_ , он сообщает нам дисперсию, объясняемую каждым выводимым признаком."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно выполнить линейный дискриминантный анализ на основе класса LinearDiscriminantAnalysis с n_components=None, чтобы вернуть коэффициент дисперсии, объясненный каждой компонентой, а затем вычислить, сколько требуется компонент, чтобы превысить некий порог дисперсии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создать объект и выполнить LDA\n",
    "lda = LinearDiscriminantAnalysis(n_components=None)\n",
    "features_lda = lda.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9912126 0.0087874]\n"
     ]
    }
   ],
   "source": [
    "#создать массив коэффициентов объясненной дисперсии\n",
    "lda_var_ratios = lda.explained_variance_ratio_\n",
    "print(lda_var_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создать функцию\n",
    "def select_n_components(var_ratio, goal_val: float) -> int:\n",
    "    #задать исходную объясненную на данный момент дисперсию\n",
    "    total_variance = 0.0\n",
    "    \n",
    "    #задать исходное количество признаков\n",
    "    n_components = 0\n",
    "    \n",
    "    #для объясненной дисперсии каждого признака:\n",
    "    for explained_variance in var_ratio:\n",
    "        #добавить объясненную дисперсию к итогу\n",
    "        total_variance += explained_variance\n",
    "        \n",
    "        #добавить единицу к количеству компонент\n",
    "        n_components += 1\n",
    "        \n",
    "        #если достигнут целевой уровень объясненной дисперсии\n",
    "        if total_variance >= goal_val:\n",
    "            #завершить цикл\n",
    "            break\n",
    "    #вернуть количество компонент       \n",
    "    return n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#выполнить функцию\n",
    "select_n_components(lda_var_ratios, 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Уменьшение количества признаков с использованием разложения матрицы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ru.wikipedia.org/wiki/%D0%9D%D0%B5%D0%BE%D1%82%D1%80%D0%B8%D1%86%D0%B0%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D0%BE%D0%B5_%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%87%D0%BD%D0%BE%D0%B5_%D1%80%D0%B0%D0%B7%D0%BB%D0%BE%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дана матрица признаков с неотрицательными значениями и требуется уменьшить ее размерность."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использовать разложение неотрицательной матрицы с целью уменьшения размерности матрицы признаков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NMF (non-negative matrix factorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загрузить библиотеки\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загрузить данные\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загрузить  матрицу признаков\n",
    "features = digits.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создать NMF и выполнить его подгонку\n",
    "nmf = NMF(n_components=10, random_state=1)\n",
    "features_nmf = nmf.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходное количество признаков:  64\n",
      "Сокращенное количество признаков:  10\n"
     ]
    }
   ],
   "source": [
    "#показать результаты\n",
    "print('Исходное количество признаков: ', features.shape[1])\n",
    "print('Сокращенное количество признаков: ', features_nmf.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разложение неотрицательной матрицы NMF является неконтролируемым методом уменьшения линейной размерности, который факторизует(разбивает на несолько матриц, произведение которых соответствует исходной матрице) - матрицу признаков в матрицы, представляющие скрытую связь между наблюдениями и их признаками."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод NMF может сократить размерность, поскольку в матричном умножении два сомножителя(умножаемые матрицы)  - могут иметь значительно меньшие размерности, чем матрица произведения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если дано желаемое число возвращаемых признаков - r, то метод NMF  разлагает матрицу признаков: V = W x H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V - наша матрица признаков d x n\n",
    "\n",
    "W - матрица в d x r\n",
    "\n",
    "H - матрица r x n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скорректировав r - можно задать объем желаемого уменьшения размерности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Требование: матрица не может содержать отрицательные значения.\n",
    "\n",
    "Метод NMF  не предоставляет объясненную дисперсию результирующих признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То лучший способ найти оптимальное значение n_components - попытаться найти в диапазоне значений то, которое дает наилучший результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Уменьшение количества признаков на разряженных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дана разряженная матриа признаков, и требуется уменьшить ее размерность."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использовать усеченное сингулярное разложение TSVD (truncated singular value decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загрузить библиотеки\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загрузить данные\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#стандартизировать матрицу признаков\n",
    "features = StandardScaler().fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#сделать разряженную матрицу\n",
    "features_sparse = csr_matrix(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#сделать объект TSVD\n",
    "tsvd = TruncatedSVD(n_components=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#выполнить TSVD на разряженной матрице\n",
    "features_sparse_tsvd = tsvd.fit(features_sparse).transform(features_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходное количество признаков:  64\n",
      "Сокращенное количество признаков:  10\n"
     ]
    }
   ],
   "source": [
    "#показать результаты\n",
    "print('Исходное количество признаков: ', features_sparse.shape[1])\n",
    "print('Сокращенное количество признаков: ', features_sparse_tsvd.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Усеченное сингулярное разложение TSVD похоже на анализ главных компонент PCA (PCA на одном из своих шагов часто используется SVD (неусеченное сингулярное разложение)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обычном сингулярном разложении при наличии d признаков создаются матрицы сомножители размера dxd, тогда как усеченное сингулярное разложение вернет сомножители, которые имеют размер n x n, где n - предварительно заданы параметром."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TSVD в отличии от  PCA работает на разряженных матрицах признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одна из проблем (TSVD) - в зависимости от того, как этот метод использует генератор случайных чисел, знаки результата могут меняться от подгонки к подгонке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простое решение использовать метод fit в конвейре предобработки - 1 раз, затем несколько раз transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и в случае линейного дискриминантного анализа, мы должны указать количество признаков (компонент), которые  мы хотим вывести. Это делается с помощью параметра n_components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но каково оптимальное количество компонент?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одна стратегия состоит в том, чтобы включить n_components как гиперпараметр для оптимизации при отборе модели( т е выбрать значение для n_components, которое производит наилучшую натренированную модель)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Альтернатива: TSVD - предоставляет нам коэффициент объясненной дисперсии каждой компоненты исходной матрицы признаков, мы можем выделить ряд компонент, которые объясняют желаемый набор дисперсии( общепринятьо использовать 95% и 99%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3003938538258395"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#суммировать коэффициенты объясненной дисперсии первых 3-х компонент\n",
    "tsvd.explained_variance_ratio_[0:3].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#это значит, первые три компоненты - объясняют 30% дисперсии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "этот процесс можно автоматизировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создать и выполнить TSVD с числом признаков меньше на 1\n",
    "tsvd = TruncatedSVD(n_components=features_sparse.shape[1]-1)\n",
    "features_tsvd = tsvd.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#поместить в список объясненные дисперсии\n",
    "tsvd_var_ratios = tsvd.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создать функцию\n",
    "def select_n_components(var_ratio, goal_val):\n",
    "    #задать исходную объясненную на данный момент дисперсию\n",
    "    total_variance = 0.0\n",
    "    \n",
    "    #задать исходное количество признаков\n",
    "    n_components = 0\n",
    "    \n",
    "    #для объясненной дисперсии каждого признака:\n",
    "    for explained_variance in var_ratio:\n",
    "        #добавить объясненную дисперсию к итогу\n",
    "        total_variance += explained_variance\n",
    "        \n",
    "        #добавить единицу к количеству компонент\n",
    "        n_components += 1\n",
    "        \n",
    "        #если достигнут целевой уровень объясненной дисперсии\n",
    "        if total_variance >= goal_val:\n",
    "            #завершить цикл\n",
    "            break\n",
    "    #вернуть количество компонент       \n",
    "    return n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_n_components(tsvd_var_ratios, 0.95)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
